{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crawler fro the netherlands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\andré\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\andré\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\andré\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Collecting selenium\n",
      "  Downloading selenium-4.19.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\andré\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.1)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.25.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\andré\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from selenium) (2024.2.2)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\andré\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from selenium) (4.11.0)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium)\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\andré\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio~=0.17->selenium)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting cffi>=1.14 (from trio~=0.17->selenium)\n",
      "  Downloading cffi-1.16.0-cp311-cp311-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pycparser (from cffi>=1.14->trio~=0.17->selenium)\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Downloading selenium-4.19.0-py3-none-any.whl (10.5 MB)\n",
      "   ---------------------------------------- 0.0/10.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/10.5 MB 4.0 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.4/10.5 MB 4.1 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.6/10.5 MB 4.4 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.8/10.5 MB 4.6 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.1/10.5 MB 4.6 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.3/10.5 MB 4.7 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.6/10.5 MB 4.9 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.9/10.5 MB 5.0 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.1/10.5 MB 5.2 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.4/10.5 MB 5.2 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.7/10.5 MB 5.3 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.9/10.5 MB 5.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.2/10.5 MB 5.4 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.5/10.5 MB 5.4 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.7/10.5 MB 5.4 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.0/10.5 MB 5.4 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.2/10.5 MB 5.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.5/10.5 MB 5.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.7/10.5 MB 5.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.0/10.5 MB 5.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.3/10.5 MB 5.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.6/10.5 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.9/10.5 MB 5.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.1/10.5 MB 5.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.3/10.5 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.5/10.5 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.8/10.5 MB 5.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.1/10.5 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.4/10.5 MB 5.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.7/10.5 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.0/10.5 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.3/10.5 MB 5.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.6/10.5 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.0/10.5 MB 5.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.3/10.5 MB 5.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.6/10.5 MB 5.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.9/10.5 MB 5.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.2/10.5 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.5/10.5 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.5/10.5 MB 5.7 MB/s eta 0:00:00\n",
      "Downloading trio-0.25.0-py3-none-any.whl (467 kB)\n",
      "   ---------------------------------------- 0.0/467.2 kB ? eta -:--:--\n",
      "   ----------------------- ---------------- 276.5/467.2 kB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 467.2/467.2 kB 5.9 MB/s eta 0:00:00\n",
      "Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "   ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 60.8/60.8 kB 3.2 MB/s eta 0:00:00\n",
      "Downloading cffi-1.16.0-cp311-cp311-win_amd64.whl (181 kB)\n",
      "   ---------------------------------------- 0.0/181.5 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 163.8/181.5 kB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 181.5/181.5 kB 5.5 MB/s eta 0:00:00\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.3/58.3 kB 3.0 MB/s eta 0:00:00\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "   ---------------------------------------- 0.0/117.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 117.6/117.6 kB 3.5 MB/s eta 0:00:00\n",
      "Installing collected packages: sortedcontainers, sniffio, pysocks, pycparser, h11, attrs, wsproto, outcome, cffi, trio, trio-websocket, selenium\n",
      "Successfully installed attrs-23.2.0 cffi-1.16.0 h11-0.14.0 outcome-1.3.0.post0 pycparser-2.22 pysocks-1.7.1 selenium-4.19.0 sniffio-1.3.1 sortedcontainers-2.4.0 trio-0.25.0 trio-websocket-0.11.1 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Base URL of the company list page\n",
    "base_url = \"https://nwba.nl\"\n",
    "\n",
    "# URL of the company list page\n",
    "company_list_url = \"https://nwba.nl/nl/leden\"\n",
    "\n",
    "# Send a GET request to the company list URL and get the response\n",
    "response = requests.get(company_list_url)\n",
    "\n",
    "# Parse the HTML content of the response using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all the <a> tags inside <div class=\"raxo-readmore\">\n",
    "company_links = soup.select('div.raxo-readmore a')\n",
    "\n",
    "# Open a CSV file for writing\n",
    "csv_file = open('data/netherlands_member.csv', 'w', newline='', encoding='utf-8')\n",
    "csv_writer = csv.DictWriter(csv_file, fieldnames=['Company Name', 'Description', 'Address', 'City', 'Postal Code', 'Contact Person', 'Email'])\n",
    "csv_writer.writeheader()\n",
    "\n",
    "# Extract the company information\n",
    "for link in company_links:\n",
    "    # Get the URL of the company page\n",
    "    company_url = base_url + link['href']\n",
    "\n",
    "    # Send a GET request to the company URL and get the response\n",
    "    company_response = requests.get(company_url)\n",
    "\n",
    "    # Parse the HTML content of the company response using BeautifulSoup\n",
    "    company_soup = BeautifulSoup(company_response.content, 'html.parser')\n",
    "\n",
    "    # Extract company description\n",
    "    company_description = company_soup.find('p', class_='content-description')\n",
    "    if company_description:\n",
    "        company_description = company_description.text.strip()\n",
    "    else:\n",
    "        company_description = None\n",
    "\n",
    "    # Extract company address details\n",
    "    address_fields = company_soup.select('ul.fields-container li')\n",
    "    address_info = {field.find('span', class_='field-label').text.strip(): field.find('span', class_='field-value').text.strip() for field in address_fields}\n",
    "\n",
    "    # Extract address, city, and postal code\n",
    "    company_name = address_info.get('Bedrijfsnaam:','')\n",
    "    address = address_info.get('Adres:', '')\n",
    "    city = address_info.get('Plaats:', '')\n",
    "    postal_code = address_info.get('Postcode:', '')\n",
    "\n",
    "    # Extract contact person and email\n",
    "    contact_person = address_info.get('Contactpersoon:', '')\n",
    "    email = address_info.get('E-mail:', '')\n",
    "\n",
    "    # Write the data to the CSV file\n",
    "    csv_writer.writerow({\n",
    "        'Company Name': company_name,\n",
    "        'Description': company_description,\n",
    "        'Address': address,\n",
    "        'City': city,\n",
    "        'Postal Code': postal_code,\n",
    "        'Contact Person': contact_person,\n",
    "        'Email': email\n",
    "    })\n",
    "\n",
    "# Close the CSV file\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Base URL of the company list page\n",
    "base_url = \"https://nwba.nl\"\n",
    "\n",
    "# URL of the company list page\n",
    "company_list_url = \"https://nwba.nl/nl/leden\"\n",
    "\n",
    "# Send a GET request to the company list URL and get the response\n",
    "response = requests.get(company_list_url)\n",
    "\n",
    "# Parse the HTML content of the response using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all the <a> tags inside <div class=\"raxo-readmore\">\n",
    "company_links = soup.select('div.raxo-readmore a')\n",
    "\n",
    "# Open a CSV file for writing\n",
    "csv_file = open('data/netherlands_member.csv', 'w', newline='', encoding='utf-8')\n",
    "csv_writer = csv.DictWriter(csv_file, fieldnames=['Company Name', 'Description', 'Address', 'City', 'Postal Code', 'Contact Person', 'Email'])\n",
    "csv_writer.writeheader()\n",
    "\n",
    "# Extract the company information\n",
    "for link in company_links:\n",
    "    # Get the URL of the company page\n",
    "    company_url = base_url + link['href']\n",
    "\n",
    "    # Send a GET request to the company URL and get the response\n",
    "    company_response = requests.get(company_url)\n",
    "\n",
    "    # Parse the HTML content of the company response using BeautifulSoup\n",
    "    company_soup = BeautifulSoup(company_response.content, 'html.parser')\n",
    "\n",
    "    # Extract company description\n",
    "    company_description = company_soup.find('p', class_='content-description')\n",
    "    if company_description:\n",
    "        company_description = company_description.text.strip()\n",
    "    else:\n",
    "        company_description = None\n",
    "\n",
    "    # Extract company address details\n",
    "    address_fields = company_soup.select('ul.fields-container li')\n",
    "    address_info = {field.find('span', class_='field-label').text.strip(): field.find('span', class_='field-value').text.strip() for field in address_fields}\n",
    "\n",
    "    # Extract address, city, and postal code\n",
    "    company_name = address_info.get('Bedrijfsnaam:','')\n",
    "    address = address_info.get('Adres:', '')\n",
    "    city = address_info.get('Plaats:', '')\n",
    "    postal_code = address_info.get('Postcode:', '')\n",
    "\n",
    "    # Extract contact person and email\n",
    "    contact_person = address_info.get('Contactpersoon:', '')\n",
    "    email = address_info.get('E-mail:', '')\n",
    "\n",
    "    # Write the data to the CSV file\n",
    "    csv_writer.writerow({\n",
    "        'Company Name': company_name,\n",
    "        'Description': company_description,\n",
    "        'Address': address,\n",
    "        'City': city,\n",
    "        'Postal Code': postal_code,\n",
    "        'Contact Person': contact_person,\n",
    "        'Email': email\n",
    "    })\n",
    "\n",
    "# Close the CSV file\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now working on a carlwer for poland: https://h2poland.eu/en/hydrogen-investment-map/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the webpage to crawl\n",
    "url = \"https://h2poland.eu/en/hydrogen-investment-map/\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all div elements with class \"col-12 col-md-6 col-lg-4 my-3\"\n",
    "div_elements = soup.find_all(\"div\", class_=\"col-12 col-md-6 col-lg-4 my-3\")\n",
    "\n",
    "# Iterate through each div element and extract the text content from child elements\n",
    "for div in div_elements:\n",
    "    # Extract text content from child elements within the div\n",
    "    text_content = [element.get_text(strip=True) for element in div.find_all([\"h3\", \"p\", \"a\"])]\n",
    "\n",
    "    # Join the text content with a newline character\n",
    "    combined_text = \"\\n\".join(text_content)\n",
    "\n",
    "    # Print the extracted text content\n",
    "    print(combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel(r'C:\\Users\\André\\Desktop\\Universität\\Universität\\Master\\Masterarbeit\\webcrawler\\data\\poland_members.xlsx')\n",
    "\n",
    "# Combine address information into a single column\n",
    "df['Full Address'] = df[['Street', 'ZIP']].apply(lambda x: ', '.join(x.dropna().astype(str)), axis=1)\n",
    "\n",
    "# Save the updated DataFrame to the Excel file\n",
    "df.to_excel(r'C:\\Users\\André\\Desktop\\Universität\\Universität\\Master\\Masterarbeit\\webcrawler\\data\\poland_members.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crawler for UK Members: https://ukhea.co.uk/industry-showcase/#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set up the Selenium webdriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# URL of the page to crawl\n",
    "url = \"https://ukhea.co.uk/industry-showcase/#\"\n",
    "\n",
    "# Navigate to the URL\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load completely\n",
    "wait = WebDriverWait(driver, 10)\n",
    "wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"elementor-column\")))\n",
    "\n",
    "# Get the HTML source of the rendered page\n",
    "html_source = driver.page_source\n",
    "\n",
    "# Save the HTML source to a file\n",
    "with open(\"ukhea_page.html\", \"w\", encoding=\"utf-8\") as html_file:\n",
    "    html_file.write(html_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company Name: 2G Energy\n",
      "Address: Unit 1 Sycamore Court, Warrington Road, Runcorn, Cheshire, WA7 1RS\n",
      "Services:\n",
      "\n",
      "Company Name: Action Sealtite\n",
      "Address: \n",
      "Services:\n",
      "\n",
      "Company Name: Acute Civil Engineering\n",
      "Address: 3 Manor Court, Salesbury Hall Road, Ribchester, PR3 3XR\n",
      "Services:\n",
      "\n",
      "Company Name: Air Products\n",
      "Address: Air Products PLC, Hersham Technology Park, Molesey Road, Hersham, KT12 4RZ\n",
      "Services:\n",
      "- Hydrogen Production\n",
      "- Fuel Handling\n",
      "- Hydrogen Refuelling Infrastructure\n",
      "- Stationary Applications\n",
      "\n",
      "Company Name: Alcazar\n",
      "Address: 7 Bell Yard, London, WC2A 2JR\n",
      "Services:\n",
      "- Sector Support Functions\n",
      "- Studies\n",
      "\n",
      "Company Name: Ames Goldsmith Ceimig\n",
      "Address: Units 1-3 Smeaton Road, Wester Gourdie Industrial Estate, Dundee, Tayside, DD2 4UT\n",
      "Services:\n",
      "\n",
      "Company Name: Anglo American\n",
      "Address: 17 Charterhouse St, London, EC1N 6RA\n",
      "Services:\n",
      "- Hydrogen Production\n",
      "- Investment\n",
      "- Transport Applications\n",
      "- Studies\n",
      "- Misc.\n",
      "\n",
      "Company Name: Ash Group\n",
      "Address: \n",
      "Services:\n",
      "\n",
      "Company Name: ATEQ\n",
      "Address: Unit 71, Washford Industrial Estate, Heming Rd, Redditch, B98 0EA\n",
      "Services:\n",
      "- Control and Equipment\n",
      "- Sector Support Functions\n",
      "\n",
      "Company Name: ATOME Energy PLC\n",
      "Address: Carrwood Park, Selby Road, Leeds, LS15 4LG\n",
      "Services:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parse the HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "\n",
    "# Find all the company sections on the page\n",
    "company_sections = soup.find_all(\"div\", class_=\"jet-listing-grid__item\")\n",
    "\n",
    "# Loop through each company section to extract information\n",
    "for company_section in company_sections:\n",
    "    # Find the company name\n",
    "    company_name = company_section.find(\"h2\", class_=\"elementor-heading-title elementor-size-default\").text.strip()\n",
    "\n",
    "    # Find the company address\n",
    "    company_address = company_section.find(\"span\", class_=\"elementor-icon-list-text\").text.strip()\n",
    "\n",
    "    # Find all the services provided by the company\n",
    "    services = [li.text.strip() for li in company_section.find_all(\"span\", class_=\"elementor-icon-list-text\")[1:]]\n",
    "\n",
    "    # Print the extracted information\n",
    "    print(f\"Company Name: {company_name}\")\n",
    "    print(f\"Address: {company_address}\")\n",
    "    print(\"Services:\")\n",
    "    for service in services:\n",
    "        print(f\"- {service}\")\n",
    "    print()\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WebDriver' object has no attribute 'find_element_by_xpath'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Loop through the alphabet and extract information from each subpage\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m letter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 39\u001b[0m     letter_element \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element_by_xpath\u001b[49m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m//a[@data-filter=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mchr\u001b[39m(letter)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     40\u001b[0m     letter_element\u001b[38;5;241m.\u001b[39mclick()\n\u001b[0;32m     41\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Wait for the page to update\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WebDriver' object has no attribute 'find_element_by_xpath'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "\n",
    "# Set up the Selenium webdriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# URL of the page to crawl\n",
    "url = \"https://ukhea.co.uk/industry-showcase/#\"\n",
    "\n",
    "# Navigate to the URL\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load completely\n",
    "wait = WebDriverWait(driver, 10)\n",
    "wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"jet-alphabet-list\")))\n",
    "\n",
    "# Find all the letter elements\n",
    "letter_elements = driver.find_elements(By.CSS_SELECTOR, \"div.jet-alphabet-list__row label\")\n",
    "\n",
    "# Loop through each letter element and click it\n",
    "for letter_element in letter_elements:\n",
    "    # Create an ActionChains instance to perform the click action\n",
    "    action = ActionChains(driver)\n",
    "    action.move_to_element(letter_element).click().perform()\n",
    "\n",
    "    # Wait for the page to update\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Get the updated HTML source\n",
    "    html_source = driver.page_source\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "\n",
    "    # Find all the company sections on the page\n",
    "    company_sections = soup.find_all(\"div\", class_=\"jet-listing-grid__item\")\n",
    "\n",
    "    # Loop through each company section to extract information\n",
    "    for company_section in company_sections:\n",
    "        # Find the company name\n",
    "        company_name = company_section.find(\"h2\", class_=\"elementor-heading-title elementor-size-default\").text.strip()\n",
    "\n",
    "        # Find the company address\n",
    "        company_address = company_section.find(\"span\", class_=\"elementor-icon-list-text\").text.strip()\n",
    "\n",
    "        # Find all the services provided by the company\n",
    "        services = [li.text.strip() for li in company_section.find_all(\"span\", class_=\"elementor-icon-list-text\")[1:]]\n",
    "\n",
    "        # Print the extracted information\n",
    "        print(f\"Company Name: {company_name}\")\n",
    "        print(f\"Address: {company_address}\")\n",
    "        print(\"Services:\")\n",
    "        for service in services:\n",
    "            print(f\"- {service}\")\n",
    "        print()\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Set up the Selenium webdriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# URL of the page to crawl\n",
    "url = \"https://ukhea.co.uk/industry-showcase/#\"\n",
    "\n",
    "# Navigate to the URL\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load completely\n",
    "wait = WebDriverWait(driver, 10)\n",
    "wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"jet-alphabet-list\")))\n",
    "\n",
    "# Find all the letter elements\n",
    "letter_elements = driver.find_elements(By.CSS_SELECTOR, \"div.jet-alphabet-list__row label\")\n",
    "\n",
    "# Function to extract company information from the page\n",
    "def extract_company_information(soup):\n",
    "    company_sections = soup.find_all(\"div\", class_=\"jet-listing-grid__item\")\n",
    "    company_data = []\n",
    "    for company_section in company_sections:\n",
    "        company_name = company_section.find(\"h2\", class_=\"elementor-heading-title elementor-size-default\").text.strip()\n",
    "        company_address = company_section.find(\"span\", class_=\"elementor-icon-list-text\").text.strip().replace(\"\\n\", \" \")\n",
    "        services = [li.text.strip() for li in company_section.find_all(\"span\", class_=\"elementor-icon-list-text\")[1:]]\n",
    "        company_data.append((company_name, company_address, services))\n",
    "    return company_data\n",
    "\n",
    "# Open a CSV file for writing\n",
    "csv_file = open('data/uk_member.csv', mode='w', newline='', encoding='utf-8')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow(['Company Name', 'Address', 'Services'])\n",
    "\n",
    "# Loop through each letter element and click it\n",
    "for letter_element in letter_elements:\n",
    "    # Create an ActionChains instance to perform the click action\n",
    "    action = ActionChains(driver)\n",
    "    action.move_to_element(letter_element).click().perform()\n",
    "\n",
    "    # Wait for the page to update\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Get the updated HTML source\n",
    "    html_source = driver.page_source\n",
    "\n",
    "    soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "    company_data = extract_company_information(soup)\n",
    "\n",
    "    # Handle cases where tuples have more than 3 values\n",
    "    for data in company_data:\n",
    "        values = list(data)\n",
    "        company_name = values[0]\n",
    "        company_address = values[1]\n",
    "        services = ', '.join(sum([values[2], []], []))  # Flatten nested list\n",
    "        csv_writer.writerow([company_name, company_address, services])\n",
    "\n",
    "# Close the CSV file and the browser\n",
    "csv_file.close()\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
