{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crawler fro the netherlands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Downloading soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "   ---------------------------------------- 0.0/147.9 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/147.9 kB ? eta -:--:--\n",
      "   ------------------- -------------------- 71.7/147.9 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 147.9/147.9 kB 1.7 MB/s eta 0:00:00\n",
      "Downloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.12.3 bs4-0.0.2 soupsieve-2.5\n",
      "Collecting selenium\n",
      "  Downloading selenium-4.19.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting urllib3<3,>=1.26 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.25.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting certifi>=2021.10.8 (from selenium)\n",
      "  Downloading certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\osi0pr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from selenium) (4.11.0)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium)\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting idna (from trio~=0.17->selenium)\n",
      "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio~=0.17->selenium)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting cffi>=1.14 (from trio~=0.17->selenium)\n",
      "  Downloading cffi-1.16.0-cp311-cp311-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pycparser (from cffi>=1.14->trio~=0.17->selenium)\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Downloading selenium-4.19.0-py3-none-any.whl (10.5 MB)\n",
      "   ---------------------------------------- 0.0/10.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/10.5 MB 2.2 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.4/10.5 MB 3.9 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.9/10.5 MB 6.2 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.5/10.5 MB 8.2 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.3/10.5 MB 9.8 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.0/10.5 MB 11.2 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.5/10.5 MB 11.3 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 4.1/10.5 MB 11.4 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 4.9/10.5 MB 12.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.7/10.5 MB 12.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.4/10.5 MB 12.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.4/10.5 MB 12.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.4/10.5 MB 12.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.1/10.5 MB 12.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.9/10.5 MB 12.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.7/10.5 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.4/10.5 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.5/10.5 MB 13.9 MB/s eta 0:00:00\n",
      "Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "   ---------------------------------------- 0.0/163.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 163.8/163.8 kB 9.6 MB/s eta 0:00:00\n",
      "Downloading trio-0.25.0-py3-none-any.whl (467 kB)\n",
      "   ---------------------------------------- 0.0/467.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 467.2/467.2 kB 14.7 MB/s eta 0:00:00\n",
      "Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "   ---------------------------------------- 0.0/121.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 121.1/121.1 kB 6.9 MB/s eta 0:00:00\n",
      "Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "   ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 60.8/60.8 kB 3.2 MB/s eta 0:00:00\n",
      "Downloading cffi-1.16.0-cp311-cp311-win_amd64.whl (181 kB)\n",
      "   ---------------------------------------- 0.0/181.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 181.5/181.5 kB 10.7 MB/s eta 0:00:00\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading idna-3.7-py3-none-any.whl (66 kB)\n",
      "   ---------------------------------------- 0.0/66.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 66.8/66.8 kB 3.8 MB/s eta 0:00:00\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.3/58.3 kB 3.2 MB/s eta 0:00:00\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "   ---------------------------------------- 0.0/117.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 117.6/117.6 kB 7.2 MB/s eta 0:00:00\n",
      "Installing collected packages: sortedcontainers, urllib3, sniffio, pysocks, pycparser, idna, h11, certifi, attrs, wsproto, outcome, cffi, trio, trio-websocket, selenium\n",
      "Successfully installed attrs-23.2.0 certifi-2024.2.2 cffi-1.16.0 h11-0.14.0 idna-3.7 outcome-1.3.0.post0 pycparser-2.22 pysocks-1.7.1 selenium-4.19.0 sniffio-1.3.1 sortedcontainers-2.4.0 trio-0.25.0 trio-websocket-0.11.1 urllib3-2.2.1 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Base URL of the company list page\n",
    "base_url = \"https://nwba.nl\"\n",
    "\n",
    "# URL of the company list page\n",
    "company_list_url = \"https://nwba.nl/nl/leden\"\n",
    "\n",
    "# Send a GET request to the company list URL and get the response\n",
    "response = requests.get(company_list_url)\n",
    "\n",
    "# Parse the HTML content of the response using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all the <a> tags inside <div class=\"raxo-readmore\">\n",
    "company_links = soup.select('div.raxo-readmore a')\n",
    "\n",
    "# Open a CSV file for writing\n",
    "csv_file = open('data/netherlands_member.csv', 'w', newline='', encoding='utf-8')\n",
    "csv_writer = csv.DictWriter(csv_file, fieldnames=['Company Name', 'Description', 'Address', 'City', 'Postal Code', 'Contact Person', 'Email'])\n",
    "csv_writer.writeheader()\n",
    "\n",
    "# Extract the company information\n",
    "for link in company_links:\n",
    "    # Get the URL of the company page\n",
    "    company_url = base_url + link['href']\n",
    "\n",
    "    # Send a GET request to the company URL and get the response\n",
    "    company_response = requests.get(company_url)\n",
    "\n",
    "    # Parse the HTML content of the company response using BeautifulSoup\n",
    "    company_soup = BeautifulSoup(company_response.content, 'html.parser')\n",
    "\n",
    "    # Extract company description\n",
    "    company_description = company_soup.find('p', class_='content-description')\n",
    "    if company_description:\n",
    "        company_description = company_description.text.strip()\n",
    "    else:\n",
    "        company_description = None\n",
    "\n",
    "    # Extract company address details\n",
    "    address_fields = company_soup.select('ul.fields-container li')\n",
    "    address_info = {field.find('span', class_='field-label').text.strip(): field.find('span', class_='field-value').text.strip() for field in address_fields}\n",
    "\n",
    "    # Extract address, city, and postal code\n",
    "    company_name = address_info.get('Bedrijfsnaam:','')\n",
    "    address = address_info.get('Adres:', '')\n",
    "    city = address_info.get('Plaats:', '')\n",
    "    postal_code = address_info.get('Postcode:', '')\n",
    "\n",
    "    # Extract contact person and email\n",
    "    contact_person = address_info.get('Contactpersoon:', '')\n",
    "    email = address_info.get('E-mail:', '')\n",
    "\n",
    "    # Write the data to the CSV file\n",
    "    csv_writer.writerow({\n",
    "        'Company Name': company_name,\n",
    "        'Description': company_description,\n",
    "        'Address': address,\n",
    "        'City': city,\n",
    "        'Postal Code': postal_code,\n",
    "        'Contact Person': contact_person,\n",
    "        'Email': email\n",
    "    })\n",
    "\n",
    "# Close the CSV file\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Base URL of the company list page\n",
    "base_url = \"https://nwba.nl\"\n",
    "\n",
    "# URL of the company list page\n",
    "company_list_url = \"https://nwba.nl/nl/leden\"\n",
    "\n",
    "# Send a GET request to the company list URL and get the response\n",
    "response = requests.get(company_list_url)\n",
    "\n",
    "# Parse the HTML content of the response using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all the <a> tags inside <div class=\"raxo-readmore\">\n",
    "company_links = soup.select('div.raxo-readmore a')\n",
    "\n",
    "# Open a CSV file for writing\n",
    "csv_file = open('data/netherlands_member.csv', 'w', newline='', encoding='utf-8')\n",
    "csv_writer = csv.DictWriter(csv_file, fieldnames=['Company Name', 'Description', 'Address', 'City', 'Postal Code', 'Contact Person', 'Email'])\n",
    "csv_writer.writeheader()\n",
    "\n",
    "# Extract the company information\n",
    "for link in company_links:\n",
    "    # Get the URL of the company page\n",
    "    company_url = base_url + link['href']\n",
    "\n",
    "    # Send a GET request to the company URL and get the response\n",
    "    company_response = requests.get(company_url)\n",
    "\n",
    "    # Parse the HTML content of the company response using BeautifulSoup\n",
    "    company_soup = BeautifulSoup(company_response.content, 'html.parser')\n",
    "\n",
    "    # Extract company description\n",
    "    company_description = company_soup.find('p', class_='content-description')\n",
    "    if company_description:\n",
    "        company_description = company_description.text.strip()\n",
    "    else:\n",
    "        company_description = None\n",
    "\n",
    "    # Extract company address details\n",
    "    address_fields = company_soup.select('ul.fields-container li')\n",
    "    address_info = {field.find('span', class_='field-label').text.strip(): field.find('span', class_='field-value').text.strip() for field in address_fields}\n",
    "\n",
    "    # Extract address, city, and postal code\n",
    "    company_name = address_info.get('Bedrijfsnaam:','')\n",
    "    address = address_info.get('Adres:', '')\n",
    "    city = address_info.get('Plaats:', '')\n",
    "    postal_code = address_info.get('Postcode:', '')\n",
    "\n",
    "    # Extract contact person and email\n",
    "    contact_person = address_info.get('Contactpersoon:', '')\n",
    "    email = address_info.get('E-mail:', '')\n",
    "\n",
    "    # Write the data to the CSV file\n",
    "    csv_writer.writerow({\n",
    "        'Company Name': company_name,\n",
    "        'Description': company_description,\n",
    "        'Address': address,\n",
    "        'City': city,\n",
    "        'Postal Code': postal_code,\n",
    "        'Contact Person': contact_person,\n",
    "        'Email': email\n",
    "    })\n",
    "\n",
    "# Close the CSV file\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now working on a carlwer for poland: https://h2poland.eu/en/hydrogen-investment-map/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the webpage to crawl\n",
    "url = \"https://h2poland.eu/en/hydrogen-investment-map/\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all div elements with class \"col-12 col-md-6 col-lg-4 my-3\"\n",
    "div_elements = soup.find_all(\"div\", class_=\"col-12 col-md-6 col-lg-4 my-3\")\n",
    "\n",
    "# Iterate through each div element and extract the text content from child elements\n",
    "for div in div_elements:\n",
    "    # Extract text content from child elements within the div\n",
    "    text_content = [element.get_text(strip=True) for element in div.find_all([\"h3\", \"p\", \"a\"])]\n",
    "\n",
    "    # Join the text content with a newline character\n",
    "    combined_text = \"\\n\".join(text_content)\n",
    "\n",
    "    # Print the extracted text content\n",
    "    print(combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel(r'C:\\Users\\André\\Desktop\\Universität\\Universität\\Master\\Masterarbeit\\webcrawler\\data\\poland_members.xlsx')\n",
    "\n",
    "# Combine address information into a single column\n",
    "df['Full Address'] = df[['Street', 'ZIP']].apply(lambda x: ', '.join(x.dropna().astype(str)), axis=1)\n",
    "\n",
    "# Save the updated DataFrame to the Excel file\n",
    "df.to_excel(r'C:\\Users\\André\\Desktop\\Universität\\Universität\\Master\\Masterarbeit\\webcrawler\\data\\poland_members.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crawler for UK Members: https://ukhea.co.uk/industry-showcase/#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we only manage to get the first page, thus we need to improve it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set up the Selenium webdriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# URL of the page to crawl\n",
    "url = \"https://ukhea.co.uk/industry-showcase/#\"\n",
    "\n",
    "# Navigate to the URL\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load completely\n",
    "wait = WebDriverWait(driver, 10)\n",
    "wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"elementor-column\")))\n",
    "\n",
    "# Get the HTML source of the rendered page\n",
    "html_source = driver.page_source\n",
    "\n",
    "# Save the HTML source to a file\n",
    "with open(\"ukhea_page.html\", \"w\", encoding=\"utf-8\") as html_file:\n",
    "    html_file.write(html_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company Name: 2G Energy\n",
      "Address: Unit 1 Sycamore Court, Warrington Road, Runcorn, Cheshire, WA7 1RS\n",
      "Services:\n",
      "\n",
      "Company Name: Action Sealtite\n",
      "Address: \n",
      "Services:\n",
      "\n",
      "Company Name: Acute Civil Engineering\n",
      "Address: 3 Manor Court, Salesbury Hall Road, Ribchester, PR3 3XR\n",
      "Services:\n",
      "\n",
      "Company Name: Air Products\n",
      "Address: Air Products PLC, Hersham Technology Park, Molesey Road, Hersham, KT12 4RZ\n",
      "Services:\n",
      "- Hydrogen Production\n",
      "- Fuel Handling\n",
      "- Hydrogen Refuelling Infrastructure\n",
      "- Stationary Applications\n",
      "\n",
      "Company Name: Alcazar\n",
      "Address: 7 Bell Yard, London, WC2A 2JR\n",
      "Services:\n",
      "- Sector Support Functions\n",
      "- Studies\n",
      "\n",
      "Company Name: Ames Goldsmith Ceimig\n",
      "Address: Units 1-3 Smeaton Road, Wester Gourdie Industrial Estate, Dundee, Tayside, DD2 4UT\n",
      "Services:\n",
      "\n",
      "Company Name: Anglo American\n",
      "Address: 17 Charterhouse St, London, EC1N 6RA\n",
      "Services:\n",
      "- Hydrogen Production\n",
      "- Investment\n",
      "- Transport Applications\n",
      "- Studies\n",
      "- Misc.\n",
      "\n",
      "Company Name: Ash Group\n",
      "Address: \n",
      "Services:\n",
      "\n",
      "Company Name: ATEQ\n",
      "Address: Unit 71, Washford Industrial Estate, Heming Rd, Redditch, B98 0EA\n",
      "Services:\n",
      "- Control and Equipment\n",
      "- Sector Support Functions\n",
      "\n",
      "Company Name: ATOME Energy PLC\n",
      "Address: Carrwood Park, Selby Road, Leeds, LS15 4LG\n",
      "Services:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parse the HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "\n",
    "# Find all the company sections on the page\n",
    "company_sections = soup.find_all(\"div\", class_=\"jet-listing-grid__item\")\n",
    "\n",
    "# Loop through each company section to extract information\n",
    "for company_section in company_sections:\n",
    "    # Find the company name\n",
    "    company_name = company_section.find(\"h2\", class_=\"elementor-heading-title elementor-size-default\").text.strip()\n",
    "\n",
    "    # Find the company address\n",
    "    company_address = company_section.find(\"span\", class_=\"elementor-icon-list-text\").text.strip()\n",
    "\n",
    "    # Find all the services provided by the company\n",
    "    services = [li.text.strip() for li in company_section.find_all(\"span\", class_=\"elementor-icon-list-text\")[1:]]\n",
    "\n",
    "    # Print the extracted information\n",
    "    print(f\"Company Name: {company_name}\")\n",
    "    print(f\"Address: {company_address}\")\n",
    "    print(\"Services:\")\n",
    "    for service in services:\n",
    "        print(f\"- {service}\")\n",
    "    print()\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we manage to get every company from all subpages, but it is not safed in a CSV yet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company Name: 2G Energy\n",
      "Address: Unit 1 Sycamore Court, Warrington Road, Runcorn, Cheshire, WA7 1RS\n",
      "Services:\n",
      "\n",
      "Company Name: Action Sealtite\n",
      "Address: \n",
      "Services:\n",
      "\n",
      "Company Name: Acute Civil Engineering\n",
      "Address: 3 Manor Court, Salesbury Hall Road, Ribchester, PR3 3XR\n",
      "Services:\n",
      "\n",
      "Company Name: Air Products\n",
      "Address: Air Products PLC, Hersham Technology Park, Molesey Road, Hersham, KT12 4RZ\n",
      "Services:\n",
      "- Hydrogen Production\n",
      "- Fuel Handling\n",
      "- Hydrogen Refuelling Infrastructure\n",
      "- Stationary Applications\n",
      "\n",
      "Company Name: Alcazar\n",
      "Address: 7 Bell Yard, London, WC2A 2JR\n",
      "Services:\n",
      "- Sector Support Functions\n",
      "- Studies\n",
      "\n",
      "Company Name: Ames Goldsmith Ceimig\n",
      "Address: Units 1-3 Smeaton Road, Wester Gourdie Industrial Estate, Dundee, Tayside, DD2 4UT\n",
      "Services:\n",
      "\n",
      "Company Name: Anglo American\n",
      "Address: 17 Charterhouse St, London, EC1N 6RA\n",
      "Services:\n",
      "- Hydrogen Production\n",
      "- Investment\n",
      "- Transport Applications\n",
      "- Studies\n",
      "- Misc.\n",
      "\n",
      "Company Name: Ash Group\n",
      "Address: \n",
      "Services:\n",
      "\n",
      "Company Name: ATEQ\n",
      "Address: Unit 71, Washford Industrial Estate, Heming Rd, Redditch, B98 0EA\n",
      "Services:\n",
      "- Control and Equipment\n",
      "- Sector Support Functions\n",
      "\n",
      "Company Name: ATOME Energy PLC\n",
      "Address: Carrwood Park, Selby Road, Leeds, LS15 4LG\n",
      "Services:\n",
      "\n",
      "Company Name: Baker Botts (UK) LLP\n",
      "Address: 41 Lothbury, London, EC2R 7HF\n",
      "Services:\n",
      "- Hydrogen Production\n",
      "- Fuel Handling\n",
      "- Hydrogen Refuelling Infrastructure\n",
      "- Control and Equipment\n",
      "- Investment\n",
      "- Stationary Applications\n",
      "- Transport Applications\n",
      "- Misc.\n",
      "\n",
      "Company Name: BOC\n",
      "Address: Forge, 43 Church Street West, Woking, Surrey, GU21 6HT\n",
      "Services:\n",
      "- Hydrogen Production\n",
      "- Fuel Handling\n",
      "- Hydrogen Refuelling Infrastructure\n",
      "- Investment\n",
      "- Stationary Applications\n",
      "- Transport Applications\n",
      "- Sector Support Functions\n",
      "- Studies\n",
      "- Misc.\n",
      "\n",
      "Company Name: Bosch\n",
      "Address: \n",
      "Services:\n",
      "\n",
      "Company Name: Boston College of Further Education\n",
      "Address: Skirbeck Road, Boston, Lincolnshire, PE21 6AF\n",
      "Services:\n",
      "\n",
      "Company Name: Bowman Power\n",
      "Address: \n",
      "Services:\n",
      "\n",
      "Company Name: BP\n",
      "Address: BP International Limited, Chertsey Road, Sunbury on Thames, Middlesex, TW16 7BP\n",
      "Services:\n",
      "- Hydrogen Production\n",
      "- Fuel Handling\n",
      "- Hydrogen Refuelling Infrastructure\n",
      "- Stationary Applications\n",
      "- Sector Support Functions\n",
      "- Misc.\n",
      "\n",
      "Company Name: Briggs Equipment UK Ltd\n",
      "Address: 7 Orbital Way, Cannock, Staffordshire WS11 8XW\n",
      "Services:\n",
      "\n",
      "Company Name: British Solar Renewables\n",
      "Address: \n",
      "Services:\n",
      "\n",
      "Company Name: Burges Salmon\n",
      "Address: One Glass Wharf, Bristol, BS2 0ZX Atria One, 144 Morrison Street, Edinburgh, EH3 8EX 6 New Street Square, London, EC4A 3BF\n",
      "Services:\n",
      "\n",
      "Company Name: Catagen\n",
      "Address: 5 Elmbank Channel Commercial Park, Queen's Road, Titanic Quarter, Belfast, BT39DT\n",
      "Services:\n",
      "- Hydrogen Production\n",
      "- Fuel Handling\n",
      "- Hydrogen Refuelling Infrastructure\n",
      "- Control and Equipment\n",
      "- Stationary Applications\n",
      "- Transport Applications\n",
      "- Studies\n",
      "- Misc.\n",
      "\n",
      "Company Name: Centrica\n",
      "Address: Millstream, Maidenhead Rd, Windsor SL4 5GD\n",
      "Services:\n",
      "- Hydrogen Production\n",
      "- Investment\n",
      "- Stationary Applications\n",
      "- Studies\n",
      "- Misc.\n",
      "\n",
      "Company Name: Ceres\n",
      "Address: \n",
      "Services:\n",
      "\n",
      "Company Name: Chesterfield Special Cylinders\n",
      "Address: Meadowhall Road, Sheffield, S9 1BT\n",
      "Services:\n",
      "\n",
      "Company Name: Clarke Energy\n",
      "Address: Unit C, Power House, Senator Point, South Boundary Road, Liverpool, L33 7RR\n",
      "Services:\n",
      "- Hydrogen Production\n",
      "- Fuel Handling\n",
      "- Hydrogen Refuelling Infrastructure\n",
      "- Stationary Applications\n",
      "- Sector Support Functions\n",
      "- Misc.\n",
      "\n",
      "Company Name: Clean Air Power\n",
      "Address: Pera Business Park, Nottingham Road, Melton Mowbray, Leicestershire, LE13 0PB\n",
      "Services:\n",
      "- Stationary Applications\n",
      "- Transport Applications\n",
      "- Sector Support Functions\n",
      "- Misc.\n",
      "\n",
      "Company Name: Clean Energy Technology\n",
      "Address: \n",
      "Services:\n",
      "\n",
      "Company Name: CMB Tech\n",
      "Address: Prospect House, Prospect Way, Hutton, Essex, CM13 1XA\n",
      "Services:\n",
      "- Fuel Handling\n",
      "- Hydrogen Refuelling Infrastructure\n",
      "- Control and Equipment\n",
      "- Stationary Applications\n",
      "- Transport Applications\n",
      "- Sector Support Functions\n",
      "- Studies\n",
      "- Misc.\n",
      "\n",
      "Company Name: Community Wind Power\n",
      "Address: \n",
      "Services:\n",
      "\n",
      "Company Name: Conduit Ventures Limited\n",
      "Address: 3-4 Devonshire Street, London W1W 5DT\n",
      "Services:\n",
      "\n",
      "Company Name: De Courcy Alexander\n",
      "Address: \n",
      "Services:\n",
      "\n",
      "Company Name: Diffusion Alloys\n",
      "Address: Teesport Commerce Park, Dockside Road, Middlesbrough, TS6 6UZ\n",
      "Services:\n",
      "- Hydrogen Production\n",
      "\n",
      "Company Name: Efectis\n",
      "Address: 307 Euston Road, London, NW1 3AD\n",
      "Services:\n",
      "- Control and Equipment\n",
      "- Sector Support Functions\n",
      "- Studies\n",
      "\n",
      "Company Name: Element Materials Technology\n",
      "Address: \n",
      "Services:\n",
      "\n",
      "Company Name: Elgar Middleton\n",
      "Address: \n",
      "Services:\n",
      "\n",
      "Company Name: Emerson\n",
      "Address: \n",
      "Services:\n",
      "- Control and Equipment\n",
      "- Sector Support Functions\n",
      "- Misc.\n",
      "\n",
      "Company Name: ENWL Maintenance and Construction\n",
      "Address: \n",
      "Services:\n",
      "\n",
      "Company Name: Equans\n",
      "Address: Equans, Neon, Q10 Quorum Business Park, Benton Lane, Newcastle Upon Tyne, NE12 8BU\n",
      "Services:\n",
      "- Sector Support Functions\n",
      "- Studies\n",
      "- Misc.\n",
      "\n",
      "Company Name: Equilibrion\n",
      "Address: Redlands, Cliftonville, Northampton, NN1 5BE\n",
      "Services:\n",
      "\n",
      "Company Name: ESI Process UK Ltd\n",
      "Address: ESI Process UK Ltd Unit 1 Lakeside House Lakeside Park Llantarnam Ind Estate Cwmbran, Torfaen NP44 3XS United Kingdom\n",
      "Services:\n",
      "- Control and Equipment\n",
      "- Sector Support Functions\n",
      "- Misc.\n",
      "\n",
      "Company Name: FAUN Zoeller\n",
      "Address: \n",
      "Services:\n",
      "\n",
      "Company Name: FBB Partners\n",
      "Address: \n",
      "Services:\n",
      "\n",
      "Company Name: Finning\n",
      "Address: \n",
      "Services:\n",
      "\n",
      "Company Name: First Hydrogen\n",
      "Address: \n",
      "Services:\n",
      "\n",
      "Company Name: Flexitallic\n",
      "Address: Scandinavia Mill, Hunsworth Lane, Cleckheaton, West Yorkshire, BD19 4LN, UK\n",
      "Services:\n",
      "- Sector Support Functions\n",
      "- Misc.\n",
      "\n",
      "Company Name: Fuel Cell Systems\n",
      "Address: \n",
      "Services:\n",
      "\n",
      "Company Name: Gen 2 Energy\n",
      "Address: Raveien 205, 3184 Borre, Norway\n",
      "Services:\n",
      "\n",
      "Company Name: GeoPura\n",
      "Address: Green Barn, Costock Road, Wysall, Nottinghamshire, NG12 5QT\n",
      "Services:\n",
      "- Hydrogen Production\n",
      "- Fuel Handling\n",
      "- Hydrogen Refuelling Infrastructure\n",
      "- Control and Equipment\n",
      "- Investment\n",
      "- Stationary Applications\n",
      "- Transport Applications\n",
      "- Sector Support Functions\n",
      "- Studies\n",
      "- Misc.\n",
      "\n",
      "Company Name: GHD\n",
      "Address: \n",
      "Services:\n",
      "\n",
      "Company Name: GM Flow Measurement Services Ltd\n",
      "Address: Unit 7, CastlePark Industrial Estate, Castle Road, Ellon, Aberdeenshire, AB41 9RF\n",
      "Services:\n",
      "- Control and Equipment\n",
      "\n",
      "Company Name: Green Cat Hydrogen\n",
      "Address: \n",
      "Services:\n",
      "\n",
      "Company Name: H2H Innovations\n",
      "Address: \n",
      "Services:\n",
      "\n",
      "Company Name: H2Transition Capital LLP\n",
      "Address: Pacific Wharf, 165 Rotherhithe Street, London, SE16 5QF\n",
      "Services:\n",
      "\n",
      "Company Name: H2V\n",
      "Address: 269 Farnborough Road, Farnborough, Hampshire, GU14 7LY\n",
      "Services:\n",
      "- Hydrogen Production\n",
      "\n",
      "Company Name: Hale Hamilton\n",
      "Address: Cowley Road, Uxbridge, Middlesex, UB8 2AF, UK\n",
      "Services:\n",
      "- Fuel Handling\n",
      "- Hydrogen Refuelling Infrastructure\n",
      "\n",
      "Company Name: Haskel\n",
      "Address: Haskel Europe Limited, North Hylton Road, Sunderland, SR5 3JD\n",
      "Services:\n",
      "- Fuel Handling\n",
      "- Hydrogen Refuelling Infrastructure\n",
      "- Control and Equipment\n",
      "- Transport Applications\n",
      "- Misc.\n",
      "\n",
      "Company Name: HiiROC\n",
      "Address: 303 National Avenue, Ideal Business Park, Kingston-upon-Hull, HU5 4JB\n",
      "Services:\n",
      "- Hydrogen Production\n",
      "- Fuel Handling\n",
      "- Hydrogen Refuelling Infrastructure\n",
      "\n",
      "Company Name: Hope Resources\n",
      "Address: Westmead House, Westmead, Farnborough, GU14 7LP\n",
      "Services:\n",
      "\n",
      "Company Name: Hydrogen Afloat\n",
      "Address: Mortimer House, 49 Church Street, Theale, Reading, RG7 5BX\n",
      "Services:\n",
      "\n",
      "Company Name: Hydrogen Safe\n",
      "Address: 1 Silk Street \n",
      "Manchester\n",
      "GREATER MANCHESTER\n",
      "M4 6LZ\n",
      "Services:\n",
      "\n",
      "Company Name: Hydrotechnik UK Test Engineering Ltd\n",
      "Address: \n",
      "Services:\n",
      "\n",
      "Company Name: ITM Power\n",
      "Address: 22 Atlas Way, Sheffield, S4 7QQ\n",
      "Services:\n",
      "- Hydrogen Production\n",
      "- Fuel Handling\n",
      "- Transport Applications\n",
      "- Misc.\n",
      "\n",
      "Company Name: Johnson Matthey\n",
      "Address: 5th Floor, 25 Farringdon Street, London, EC4A 4AB\n",
      "Services:\n",
      "- Hydrogen Production\n",
      "- Sector Support Functions\n",
      "- Misc.\n",
      "\n",
      "Company Name: Kiwa Energy\n",
      "Address: Kiwa House, Malvern View Business Park, Bishops Cleeve, Cheltenham, GL52 7DQ\n",
      "Services:\n",
      "- Hydrogen Production\n",
      "- Fuel Handling\n",
      "- Hydrogen Refuelling Infrastructure\n",
      "- Control and Equipment\n",
      "- Stationary Applications\n",
      "- Transport Applications\n",
      "- Sector Support Functions\n",
      "- Studies\n",
      "- Misc.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m action\u001b[38;5;241m.\u001b[39mmove_to_element(letter_element)\u001b[38;5;241m.\u001b[39mclick()\u001b[38;5;241m.\u001b[39mperform()\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Wait for the page to update\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Get the updated HTML source\u001b[39;00m\n\u001b[0;32m     36\u001b[0m html_source \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mpage_source\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "\n",
    "# Set up the Selenium webdriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# URL of the page to crawl\n",
    "url = \"https://ukhea.co.uk/industry-showcase/#\"\n",
    "\n",
    "# Navigate to the URL\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load completely\n",
    "wait = WebDriverWait(driver, 10)\n",
    "wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"jet-alphabet-list\")))\n",
    "\n",
    "# Find all the letter elements\n",
    "letter_elements = driver.find_elements(By.CSS_SELECTOR, \"div.jet-alphabet-list__row label\")\n",
    "\n",
    "# Loop through each letter element and click it\n",
    "for letter_element in letter_elements:\n",
    "    # Create an ActionChains instance to perform the click action\n",
    "    action = ActionChains(driver)\n",
    "    action.move_to_element(letter_element).click().perform()\n",
    "\n",
    "    # Wait for the page to update\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Get the updated HTML source\n",
    "    html_source = driver.page_source\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "\n",
    "    # Find all the company sections on the page\n",
    "    company_sections = soup.find_all(\"div\", class_=\"jet-listing-grid__item\")\n",
    "\n",
    "    # Loop through each company section to extract information\n",
    "    for company_section in company_sections:\n",
    "        # Find the company name\n",
    "        company_name = company_section.find(\"h2\", class_=\"elementor-heading-title elementor-size-default\").text.strip()\n",
    "\n",
    "        # Find the company address\n",
    "        company_address = company_section.find(\"span\", class_=\"elementor-icon-list-text\").text.strip()\n",
    "\n",
    "        # Find all the services provided by the company\n",
    "        services = [li.text.strip() for li in company_section.find_all(\"span\", class_=\"elementor-icon-list-text\")[1:]]\n",
    "\n",
    "        # Print the extracted information\n",
    "        print(f\"Company Name: {company_name}\")\n",
    "        print(f\"Address: {company_address}\")\n",
    "        print(\"Services:\")\n",
    "        for service in services:\n",
    "            print(f\"- {service}\")\n",
    "        print()\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we finally get to safe everything in a csv, extract all informations and format it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "# Set up the Selenium webdriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# URL of the page to crawl\n",
    "url = \"https://ukhea.co.uk/industry-showcase/#\"\n",
    "\n",
    "# Navigate to the URL\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load completely\n",
    "wait = WebDriverWait(driver, 10)\n",
    "wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"jet-alphabet-list\")))\n",
    "\n",
    "# Find all the letter elements\n",
    "letter_elements = driver.find_elements(By.CSS_SELECTOR, \"div.jet-alphabet-list__row label\")\n",
    "\n",
    "# Function to extract company information from the page\n",
    "def extract_company_information(soup):\n",
    "    company_sections = soup.find_all(\"div\", class_=\"jet-listing-grid__item\")\n",
    "    company_data = []\n",
    "    for company_section in company_sections:\n",
    "        company_name = company_section.find(\"h2\", class_=\"elementor-heading-title elementor-size-default\").text.strip()\n",
    "        company_address = company_section.find(\"span\", class_=\"elementor-icon-list-text\").text.strip().replace(\"\\n\", \" \")\n",
    "        services = [li.text.strip() for li in company_section.find_all(\"span\", class_=\"elementor-icon-list-text\")[1:]]\n",
    "        company_data.append((company_name, company_address, services))\n",
    "    return company_data\n",
    "\n",
    "# Open a CSV file for writing\n",
    "csv_file = open('data/uk_membertest.csv', mode='w', newline='', encoding='utf-8')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow(['Company Name', 'Address', 'Services'])\n",
    "\n",
    "# Loop through each letter element and click it\n",
    "for letter_element in letter_elements:\n",
    "    # Create an ActionChains instance to perform the click action\n",
    "    action = ActionChains(driver)\n",
    "    action.move_to_element(letter_element).click().perform()\n",
    "\n",
    "    # Wait for the page to update\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Get the updated HTML source\n",
    "    html_source = driver.page_source\n",
    "\n",
    "    soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "    company_data = extract_company_information(soup)\n",
    "\n",
    "    # Handle cases where tuples have more than 3 values\n",
    "    for data in company_data:\n",
    "        values = list(data)\n",
    "        company_name = values[0]\n",
    "        company_address = values[1]\n",
    "        services = ', '.join(sum([values[2], []], []))  # Flatten nested list\n",
    "        csv_writer.writerow([company_name, company_address, services])\n",
    "\n",
    "# Close the CSV file and the browser\n",
    "csv_file.close()\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we work on a crawler for the french company data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
